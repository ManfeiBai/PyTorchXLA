diff --git a/aten/src/ATen/Context.h b/aten/src/ATen/Context.h
index ff75ba3a7e3..5089abdb3af 100644
--- a/aten/src/ATen/Context.h
+++ b/aten/src/ATen/Context.h
@@ -14,6 +14,7 @@
 #include <ATen/detail/HIPHooksInterface.h>
 #include <ATen/detail/ComplexHooksInterface.h>
 #include <c10/util/Exception.h>
+#include <c10/core/impl/DeviceGuardImplInterface.h>
 
 #include <memory>
 #include <mutex>
@@ -76,6 +77,9 @@ class CAFFE2_API Context {
   bool hasHIP() const {
     return detail::getHIPHooks().hasHIP();
   }
+  bool hasXLA() const {
+    return c10::impl::hasDeviceGuardImpl(at::DeviceType::XLA);
+  }
   // defined in header so that getNonVariableType has ability to inline
   // call_once check. getNonVariableType is called fairly frequently
   THCState* lazyInitCUDA() {
@@ -204,14 +208,28 @@ static inline bool hasHIP() {
   return globalContext().hasHIP();
 }
 
+static inline bool hasXLA() {
+  return globalContext().hasXLA();
+}
+
+// Despite its name, this function returns the number of *CUDA* GPUs.
 static inline size_t getNumGPUs() {
-  if (hasCUDA()) {
+  // WARNING: DO NOT ADD LOGIC TO HANDLE OTHER DEVICE TYPES TO THIS
+  // FUNCTION.  If you are interested in interrogating the number of
+  // devices for a specific device type, add that function to the
+  // relevant library (e.g., similar to at::cuda::device_count())
+  if (hasCUDA() && hasHIP()) {
+    throw std::runtime_error(
+        "Enabling both CUDA and HIP in ATen is not supported, as HIP masquerades "
+        "to be CUDA (e.g., when you say CUDA, on a HIP build of ATen, this actually "
+        "means HIP.  Rebuild PyTorch with one or the other disabled.");
+  } else if (hasCUDA()) {
     return detail::getCUDAHooks().getNumGPUs();
-  }
-  if (hasHIP()) {
+  } else if (hasHIP()) {
     return detail::getHIPHooks().getNumGPUs();
+  } else {
+    return 0;
   }
-  return 0;
 }
 
 static inline bool hasOpenMP() {
diff --git a/c10/core/impl/DeviceGuardImplInterface.h b/c10/core/impl/DeviceGuardImplInterface.h
index f19b99670b4..66b1dffa7ee 100644
--- a/c10/core/impl/DeviceGuardImplInterface.h
+++ b/c10/core/impl/DeviceGuardImplInterface.h
@@ -131,4 +131,8 @@ inline const DeviceGuardImplInterface* getDeviceGuardImpl(DeviceType type) {
   return p;
 }
 
+inline bool hasDeviceGuardImpl(DeviceType type) {
+  return device_guard_impl_registry[static_cast<size_t>(type)].load();
+}
+
 }} // namespace c10::impl
diff --git a/test/cpp_extensions/msnpu_extension.cpp b/test/cpp_extensions/msnpu_extension.cpp
index 48acd643adf..6336e5e238f 100644
--- a/test/cpp_extensions/msnpu_extension.cpp
+++ b/test/cpp_extensions/msnpu_extension.cpp
@@ -9,7 +9,7 @@ static int test_int;
 Tensor get_dtype_tensor(caffe2::TypeMeta dtype) {
   auto tensor_impl = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(
       Storage(
-          dtype, 0, at::DataPtr(nullptr, Device(DeviceType::MSNPU, 1)), nullptr, false),
+          dtype, 0, at::DataPtr(nullptr, Device(DeviceType::MSNPU, 0)), nullptr, false),
       MSNPUTensorId(),
       false);
   return Tensor(std::move(tensor_impl));
@@ -91,6 +91,45 @@ void init_msnpu_extension() {
     &ones_like_override);
 }
 
+// TODO: Extend this to exercise multi-device setting.  In that case,
+// we need to add a thread local variable to track the current device.
+struct MSNPUGuardImpl final : public c10::impl::DeviceGuardImplInterface {
+  static constexpr DeviceType static_type = DeviceType::MSNPU;
+  MSNPUGuardImpl() {}
+  MSNPUGuardImpl(DeviceType t) {
+    AT_ASSERT(t == DeviceType::MSNPU);
+  }
+  DeviceType type() const override {
+    return DeviceType::MSNPU;
+  }
+  Device exchangeDevice(Device d) const override {
+    AT_ASSERT(d.type() == DeviceType::MSNPU);
+    AT_ASSERT(d.index() == 0);
+    return d;
+  }
+  Device getDevice() const override {
+    return Device(DeviceType::MSNPU, 0);
+  }
+  void setDevice(Device d) const override {
+    AT_ASSERT(d.type() == DeviceType::MSNPU);
+    AT_ASSERT(d.index() == 0);
+  }
+  void uncheckedSetDevice(Device d) const noexcept override {
+  }
+  Stream getStream(Device d) const noexcept override {
+    return Stream(Stream::DEFAULT, Device(DeviceType::MSNPU, 0));
+  }
+  Stream exchangeStream(Stream s) const noexcept override {
+    return Stream(Stream::DEFAULT, Device(DeviceType::MSNPU, 0));
+  }
+  DeviceIndex deviceCount() const override {
+    return 1;
+  }
+};
+
+constexpr DeviceType MSNPUGuardImpl::static_type;
+C10_REGISTER_GUARD_IMPL(MSNPU, MSNPUGuardImpl);
+
 int get_test_int() {
   return test_int;
 }
diff --git a/test/test_cpp_extensions.py b/test/test_cpp_extensions.py
index e07b823c358..667f6fe8b66 100755
--- a/test/test_cpp_extensions.py
+++ b/test/test_cpp_extensions.py
@@ -639,7 +639,7 @@ def test_zeros(self):
         self.assertEqual(a.sum(), 0)
 
         b = torch.zeros(5, 5, device='msnpu')
-        self.assertEqual(b.device, torch.device('msnpu', 1))
+        self.assertEqual(b.device, torch.device('msnpu', 0))
         self.assertEqual(msnpu_extension.get_test_int(), 0)
         self.assertEqual(torch.get_default_dtype(), b.dtype)
 
diff --git a/torch/csrc/autograd/engine.cpp b/torch/csrc/autograd/engine.cpp
index 44ab9c535ae..9c8e57f3e8a 100644
--- a/torch/csrc/autograd/engine.cpp
+++ b/torch/csrc/autograd/engine.cpp
@@ -203,22 +203,45 @@ Engine::Engine() = default;
 // This Engine's ReadyQueues and their corresponding threads are leaked here
 Engine::~Engine() = default;
 
-// TODO: Engine is not written in a way that it can deal with anything that's
-// not CUDA.
 auto Engine::thread_init(int device) -> void {
   THInferNumThreads();
+  // Note [Allocating GPUs to autograd threads]
+  // ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+  // What's our strategy here?  Originally, the autograd engine was written
+  // with only CUDA in mind.  We allocate one thread to handle all CPU
+  // operations, and a thread per CUDA device.
+  //
+  // But what if we have OTHER devices?  There are two plausible
+  // strategies:
+  //
+  //  - We can allocate threads equal to max(num_cuda_devices, num_xla_devices,
+  //    ...) and colocate cuda device 0 with xla device 0
+  //  - We can allocate threads equal to sum(num_cuda_devices, num_xla_devices,
+  //    ...) keeping everyone separate.
+  //
+  // We don't have any good reason to prefer one or the other, but for
+  // historical reasons we refer to devices by int and not Device, so
+  // the former is more convenient to do, and so that's what we do.
+  //
   // NB: We MUST NOT construct the guard for device -1,
   // as in some settings we compile with cuda, but
   // have lazy stubs for CUDA functionality (so actually
   // attempting to setup a guard(-1) will cause an
   // error, because it will still query cudaGetDevice).
-  at::OptionalDeviceGuard guard;
+  // NB: These are not OptionalCUDAGuard/etc because engine.cpp
+  // is built as part of the CPU-only library; so we need to
+  // dynamic dispatch.
+  // NB: We need an array here since neither DeviceGuard nor OptionalDeviceGuard
+  // are movable.
+  std::array<c10::OptionalDeviceGuard,
+             static_cast<size_t>(c10::DeviceType::COMPILE_TIME_MAX_DEVICE_TYPES)>
+      guards;  // Guards! Guards!
   if (device != -1) {
-    if (at::hasCUDA()) {
-      guard.reset_device(at::Device(at::DeviceType::CUDA, device));
-    }
-    if (at::hasHIP()) {
-      guard.reset_device(at::Device(at::DeviceType::HIP, device));
+    for (size_t i = 0; i < static_cast<size_t>(c10::DeviceType::COMPILE_TIME_MAX_DEVICE_TYPES); i++) {
+      auto* impl = c10::impl::device_guard_impl_registry[i].load();
+      if (impl && device < impl->deviceCount()) {
+        guards[i].reset_device(at::Device(static_cast<c10::DeviceType>(i), device));
+      }
     }
   }
   worker_device = device;
@@ -352,7 +375,7 @@ static void validate_outputs(const edge_list& edges, variable_list& grads, const
       ss << metadata.type() << " but got " << grads[i].type();
       AT_ERROR(format_error(ss.str()));
     }
-    const auto output_device = output.is_cuda() ? output.get_device() : -1;
+    const int output_device = unsound_get_device_idx(output);
     if (output_device != metadata.device()) {
       std::stringstream ss;
       ss << "invalid gradient at index " << i << " - expected device ";
@@ -632,8 +655,17 @@ auto Engine::ready_queue(int device) -> ReadyQueue& {
 }
 
 auto Engine::start_threads() -> void {
-  int num_devices = at::getNumGPUs();
-  // One for CPU, plus one for every GPU device
+  // See Note [Allocating GPUs to autograd threads]
+  c10::DeviceIndex num_devices = 0;
+  for (size_t i = 0; i < static_cast<size_t>(c10::DeviceType::COMPILE_TIME_MAX_DEVICE_TYPES); i++) {
+    auto* impl = c10::impl::device_guard_impl_registry[i].load();
+    if (impl) {
+      num_devices = std::max(num_devices, impl->deviceCount());
+    }
+  }
+
+  // One for CPU, plus one for every GPU device (but colocate GPUs of different
+  // types)
   int num_threads = num_devices + 1;
   ready_queues = std::vector<std::shared_ptr<ReadyQueue>>(num_threads);
   for (auto& queue : ready_queues)
diff --git a/torch/csrc/autograd/input_buffer.cpp b/torch/csrc/autograd/input_buffer.cpp
index 5322bee67fa..1ee1879fb56 100644
--- a/torch/csrc/autograd/input_buffer.cpp
+++ b/torch/csrc/autograd/input_buffer.cpp
@@ -1,6 +1,7 @@
 #include <torch/csrc/autograd/input_buffer.h>
 
 #include <torch/csrc/autograd/functions/basic_ops.h>
+#include <torch/csrc/autograd/input_metadata.h>
 
 #include <ATen/DeviceGuard.h>
 
@@ -42,10 +43,15 @@ void InputBuffer::add(size_t pos, Variable var) {
 
 auto InputBuffer::device() const -> int {
   for (auto& var : buffer) {
-    if (var.defined() && var.is_cuda()) {
-      return var.get_device();
+    if (var.defined()) {
+      int device = unsound_get_device_idx(var);
+      if (device >= 0) {
+        return device;
+      }
     }
   }
+  // Only report to the CPU thread if there really were no tensors
+  // from other devices.
   return -1;
 }
 
diff --git a/torch/csrc/autograd/input_metadata.h b/torch/csrc/autograd/input_metadata.h
index 678e7a1edb8..9ea035e1dd8 100644
--- a/torch/csrc/autograd/input_metadata.h
+++ b/torch/csrc/autograd/input_metadata.h
@@ -6,6 +6,16 @@
 
 namespace torch { namespace autograd {
 
+// This legacy function squashes a tensor's Device into an integer
+// index which identifies which autograd thread it refers to.
+inline int unsound_get_device_idx(const at::Tensor& tensor) {
+  if (tensor.device().type() == at::DeviceType::CPU) {
+    return -1;
+  } else {
+    return tensor.device().index();
+  }
+}
+
 /// A tensor's type and shape. Each Function records the required type and
 /// shape of its inputs. If is_valid() is false, then the corresponding input
 /// is not used and may be an undefined tensor.
@@ -16,7 +26,7 @@ struct InputMetadata {
   : type_{&type} , shape_{shape}, device_{device} { }
 
   InputMetadata(const at::Tensor& t)
-  : InputMetadata(t.type(), t.sizes(), t.is_cuda() ? t.get_device() : - 1) { }
+  : InputMetadata(t.type(), t.sizes(), unsound_get_device_idx(t)) { }
 
   bool is_valid() const {
     return type_ != nullptr;
@@ -42,6 +52,14 @@ struct InputMetadata {
 private:
   const at::Type* type_ = nullptr;
   at::DimVector shape_;
+  // This should be a 'Device'.  However, it is not for hysterical raisins,
+  // when we only had CPU and CUDA device (so we let -1 be CPU and other
+  // numbers be CUDA).  We haven't had time to refactor this properly,
+  // but we needed to support other devices like XLA, so what this really
+  // is, is an approximation of the Device, but with the device type dropped.
+  // This is good enough for us to assign this to autograd threads, but
+  // it's not good enough for us to do completely correct checks that device
+  // lines up.  When we fix this to be 'Device', you can delete this comment.
   const int64_t device_ = -1;
 };
 
diff --git a/torch/csrc/autograd/variable.cpp b/torch/csrc/autograd/variable.cpp
index 44bd52b72f7..56eb6406dc0 100644
--- a/torch/csrc/autograd/variable.cpp
+++ b/torch/csrc/autograd/variable.cpp
@@ -164,7 +164,10 @@ void Variable::Impl::set_data(const at::Tensor &new_data) {
   auto prior_accumulator = autograd_meta->grad_accumulator_.lock();
   if (prior_accumulator) {
     const auto prior_device = prior_accumulator->input_metadata(0).device();
-    const auto new_device = new_data.is_cuda() ? new_data.get_device() : -1;
+    // TODO: THIS IS WRONG.  WE WILL FAIL TO DETECT IF A GRAD ACCUMULATOR
+    // MOVES FROM CUDA DEVICE 1 TO XLA DEVICE 1.  When you fix metadata
+    // to store a real Device you will be able to do this check correctly.
+    const auto new_device = unsound_get_device_idx(new_data);
 
     if (new_data.type() != data_.type() || prior_device != new_device) {
       autograd_meta->grad_accumulator_.reset();
@@ -217,7 +220,9 @@ const std::shared_ptr<Function>& Variable::grad_fn() const {
       fn->add_input_metadata(
         diff_view_meta->base_.type()
       , sizes() // Note: sizes(), not base_.sizes(), is intentional
-      , diff_view_meta->base_.is_cuda() ? diff_view_meta->base_.get_device() : -1);
+      // TODO: stop throwing away useful device information here!
+      // Delay the autograd thread assignment to later.
+      , unsound_get_device_idx(diff_view_meta->base_));
       diff_view_meta->grad_fn_ = std::move(fn);
       diff_view_meta->attr_version = current_version;
     }
