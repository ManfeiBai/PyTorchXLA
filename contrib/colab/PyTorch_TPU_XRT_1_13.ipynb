{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch TPU XRT 1.13",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "fdBz0cmnDkLJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Copyright 2019 Google LLC.\n",
        "SPDX-License-Identifier: Apache-2.0"
      ]
    },
    {
      "metadata": {
        "id": "IQG3y7IGvc7o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "ab589c70-1f36-470e-ff01-1f6040180554"
      },
      "cell_type": "code",
      "source": [
        "!pip install \\\n",
        "  http://storage.googleapis.com/pytorch-tpu-releases/tf-1.13/torch-1.0.0a0+1d94a2b-cp36-cp36m-linux_x86_64.whl  \\\n",
        "  http://storage.googleapis.com/pytorch-tpu-releases/tf-1.13/torch_xla-0.1+5622d42-cp36-cp36m-linux_x86_64.whl"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==1.0.0a0+1d94a2b from http://storage.googleapis.com/pytorch-tpu-releases/tf-1.13/torch-1.0.0a0+1d94a2b-cp36-cp36m-linux_x86_64.whl in /usr/local/lib/python3.6/dist-packages (1.0.0a0+1d94a2b)\n",
            "Requirement already satisfied: torch-xla==0.1+5622d42 from http://storage.googleapis.com/pytorch-tpu-releases/tf-1.13/torch_xla-0.1+5622d42-cp36-cp36m-linux_x86_64.whl in /usr/local/lib/python3.6/dist-packages (0.1+5622d42)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "brEkCkFI-Hmy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch_xla\n",
        "\n",
        "class XlaMulAdd(nn.Module):                                                                                             \n",
        "  def forward(self, x, y):                                                                                            \n",
        "    return x * y + y                                                                                                \n",
        "\n",
        "# Inputs and output to/from XLA models are always in replicated mode. The shapes\n",
        "# are [NUM_REPLICAS][NUM_VALUES]. A non replicated, single core, execution will\n",
        "# has NUM_REPLICAS == 1, but retain the same shape rank.                                                                                                                               \n",
        "x = torch.rand(3, 5)                                                                                                    \n",
        "y = torch.rand(3, 5)                                                                                                    \n",
        "model = XlaMulAdd()                                                                                                     \n",
        "traced_model = torch.jit.trace(model, (x, y))                                                                             \n",
        "xla_model = torch_xla._XLAC.XlaModule(traced_model)                                                             \n",
        "output_xla = xla_model((torch_xla._XLAC.XLATensor(x), torch_xla._XLAC.XLATensor(y)))                                               \n",
        "expected = model(x, y)\n",
        "print(output_xla[0][0].to_tensor().data)\n",
        "print(expected.data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PM8Iljy-OXhx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4683
        },
        "outputId": "26ad236f-7cb0-4778-9042-1bbc1100d69a"
      },
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import torch_xla\n",
        "import torch_xla_py.utils as xu\n",
        "import torch_xla_py.xla_model as xm\n",
        "import unittest\n",
        "import sys\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "parser = argparse.ArgumentParser(add_help=False)\n",
        "parser.add_argument('--datadir', type=str, default='/tmp/mnist-data')\n",
        "parser.add_argument('--logdir', type=str, default='/tmp/logs')\n",
        "parser.add_argument('--num_cores', type=int, default=1)\n",
        "parser.add_argument('--batch_size', type=int, default=512)\n",
        "parser.add_argument('--num_epochs', type=int, default=10)\n",
        "parser.add_argument('--num_workers', type=int, default=4)\n",
        "parser.add_argument('--target_accuracy', type=float, default=98.0)\n",
        "parser.add_argument('--fake_data', action='store_true')\n",
        "parser.add_argument('--tidy', action='store_true')\n",
        "parser.add_argument('--metrics_debug', action='store_true')\n",
        "\n",
        "FLAGS, leftovers = parser.parse_known_args()\n",
        "sys.argv = [sys.argv[0]] + leftovers\n",
        "# Setup import folders.\n",
        "xla_folder = os.path.dirname(os.path.dirname(os.path.abspath(sys.argv[0])))\n",
        "print('xla folder', xla_folder)\n",
        "sys.path.append(os.path.join(os.path.dirname(xla_folder), 'test'))\n",
        "sys.path.insert(0, xla_folder)\n",
        "\n",
        "class MNIST(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(MNIST, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "    self.bn1 = nn.BatchNorm2d(10)\n",
        "    self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "    self.bn2 = nn.BatchNorm2d(20)\n",
        "    self.fc1 = nn.Linear(320, 50)\n",
        "    self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "    x = self.bn1(x)\n",
        "    x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
        "    x = self.bn2(x)\n",
        "    x = x.view(-1, 320)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.fc2(x)\n",
        "    return F.log_softmax(x, dim=1)\n",
        "\n",
        "print (\"mnist defined\")\n",
        "def train_mnist():\n",
        "  assert FLAGS.num_cores == 1\n",
        "  torch.manual_seed(1)\n",
        "  # Training settings\n",
        "  lr = 0.01\n",
        "  momentum = 0.5\n",
        "  log_interval = 5\n",
        "\n",
        "  if FLAGS.fake_data:\n",
        "    print('using fake data')\n",
        "    train_loader = xu.SampleGenerator(\n",
        "        data=torch.zeros(FLAGS.batch_size, 1, 28, 28),\n",
        "        target=torch.zeros(FLAGS.batch_size, dtype=torch.int64),\n",
        "        sample_count=60000 // FLAGS.batch_size)\n",
        "    test_loader = xu.SampleGenerator(\n",
        "        data=torch.zeros(FLAGS.batch_size, 1, 28, 28),\n",
        "        target=torch.zeros(FLAGS.batch_size, dtype=torch.int64),\n",
        "        sample_count=10000 // FLAGS.batch_size)\n",
        "  else:\n",
        "    print('using real data', FLAGS.datadir)\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST(\n",
        "            FLAGS.datadir,\n",
        "            train=True,\n",
        "            download=True,\n",
        "            transform=transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.1307,), (0.3081,))\n",
        "            ])),\n",
        "        batch_size=FLAGS.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=FLAGS.num_workers)\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST(\n",
        "            FLAGS.datadir,\n",
        "            train=False,\n",
        "            transform=transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.1307,), (0.3081,))\n",
        "            ])),\n",
        "        batch_size=FLAGS.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=FLAGS.num_workers)\n",
        "\n",
        "  model = MNIST()\n",
        "\n",
        "  inputs = torch.zeros(FLAGS.batch_size, 1, 28, 28)\n",
        "  xla_model = xm.XlaModel(model, [inputs])\n",
        "  optimizer = optim.SGD(xla_model.parameters_list(), lr=lr, momentum=momentum)\n",
        "  loss_fn = nn.NLLLoss()\n",
        "  accuracy = None\n",
        "  for epoch in range(1, FLAGS.num_epochs + 1):\n",
        "    # Training loop for epoch.\n",
        "    start_time = time.time()\n",
        "    processed = 0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "      if data.size()[0] != FLAGS.batch_size:\n",
        "        break\n",
        "      optimizer.zero_grad()\n",
        "      y = xla_model(data)\n",
        "      y[0].requires_grad = True\n",
        "      loss = loss_fn(y[0], target)\n",
        "      loss.backward()\n",
        "      xla_model.backward(y)\n",
        "      optimizer.step()\n",
        "      processed += FLAGS.batch_size\n",
        "      if batch_idx % log_interval == 0:\n",
        "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\t'\n",
        "              'Loss: {:.6f}\\tSamples/sec: {:.1f}'.format(\n",
        "                  epoch, processed,\n",
        "                  len(train_loader) * FLAGS.batch_size,\n",
        "                  100. * batch_idx / len(train_loader), loss,\n",
        "                  processed / (time.time() - start_time)))\n",
        "\n",
        "    # Eval loop for epoch.\n",
        "    start_time = time.time()\n",
        "    correct_count = 0\n",
        "    test_loss = 0\n",
        "    count = 0\n",
        "    for batch_idx, (data, target) in enumerate(test_loader):\n",
        "      if data.size()[0] != FLAGS.batch_size:\n",
        "        break\n",
        "      y = xla_model(data)\n",
        "      test_loss += loss_fn(y[0], target).sum().item()\n",
        "      pred = y[0].max(1, keepdim=True)[1]\n",
        "      correct_count += pred.eq(target.view_as(pred)).sum().item()\n",
        "      count += FLAGS.batch_size\n",
        "\n",
        "    test_loss /= count\n",
        "    accuracy = 100.0 * correct_count / count\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%), '\n",
        "          'Samples/sec: {:.1f}\\n'.format(test_loss, correct_count, count,\n",
        "                                         accuracy,\n",
        "                                         count / (time.time() - start_time)))\n",
        "    # Debug metric dumping.\n",
        "    if FLAGS.metrics_debug:\n",
        "      print(torch_xla._XLAC._xla_metrics_report())\n",
        "\n",
        "  return accuracy\n",
        "\n",
        "torch.set_default_tensor_type('torch.FloatTensor')\n",
        "train_mnist()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "xla folder /usr/local/lib/python3.6\n",
            "mnist defined\n",
            "using real data /tmp/mnist-data\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n",
            "Train Epoch: 1 [512/60416 (0%)]\tLoss: 2.335922\tSamples/sec: 164.9\n",
            "Train Epoch: 1 [3072/60416 (4%)]\tLoss: 2.280061\tSamples/sec: 881.7\n",
            "Train Epoch: 1 [5632/60416 (8%)]\tLoss: 2.234043\tSamples/sec: 1491.1\n",
            "Train Epoch: 1 [8192/60416 (13%)]\tLoss: 2.184287\tSamples/sec: 2008.5\n",
            "Train Epoch: 1 [10752/60416 (17%)]\tLoss: 2.137959\tSamples/sec: 2513.4\n",
            "Train Epoch: 1 [13312/60416 (21%)]\tLoss: 2.088081\tSamples/sec: 2949.1\n",
            "Train Epoch: 1 [15872/60416 (25%)]\tLoss: 2.045442\tSamples/sec: 3363.5\n",
            "Train Epoch: 1 [18432/60416 (30%)]\tLoss: 1.982768\tSamples/sec: 3749.5\n",
            "Train Epoch: 1 [20992/60416 (34%)]\tLoss: 1.934063\tSamples/sec: 4085.6\n",
            "Train Epoch: 1 [23552/60416 (38%)]\tLoss: 1.863042\tSamples/sec: 4427.6\n",
            "Train Epoch: 1 [26112/60416 (42%)]\tLoss: 1.812325\tSamples/sec: 4727.7\n",
            "Train Epoch: 1 [28672/60416 (47%)]\tLoss: 1.748191\tSamples/sec: 5028.8\n",
            "Train Epoch: 1 [31232/60416 (51%)]\tLoss: 1.717543\tSamples/sec: 5302.1\n",
            "Train Epoch: 1 [33792/60416 (55%)]\tLoss: 1.663676\tSamples/sec: 5495.5\n",
            "Train Epoch: 1 [36352/60416 (59%)]\tLoss: 1.592082\tSamples/sec: 5653.9\n",
            "Train Epoch: 1 [38912/60416 (64%)]\tLoss: 1.568489\tSamples/sec: 5788.2\n",
            "Train Epoch: 1 [41472/60416 (68%)]\tLoss: 1.477393\tSamples/sec: 5903.1\n",
            "Train Epoch: 1 [44032/60416 (72%)]\tLoss: 1.407723\tSamples/sec: 5971.1\n",
            "Train Epoch: 1 [46592/60416 (76%)]\tLoss: 1.376249\tSamples/sec: 6100.4\n",
            "Train Epoch: 1 [49152/60416 (81%)]\tLoss: 1.319375\tSamples/sec: 6197.8\n",
            "Train Epoch: 1 [51712/60416 (85%)]\tLoss: 1.204005\tSamples/sec: 6315.6\n",
            "Train Epoch: 1 [54272/60416 (89%)]\tLoss: 1.212593\tSamples/sec: 6171.3\n",
            "Train Epoch: 1 [56832/60416 (93%)]\tLoss: 1.129196\tSamples/sec: 6284.7\n",
            "Train Epoch: 1 [59392/60416 (97%)]\tLoss: 1.060781\tSamples/sec: 6426.6\n",
            "\n",
            "Test set: Average loss: 0.0020, Accuracy: 7870/9728 (80.90%), Samples/sec: 8403.3\n",
            "\n",
            "Train Epoch: 2 [512/60416 (0%)]\tLoss: 1.054674\tSamples/sec: 1262.7\n",
            "Train Epoch: 2 [3072/60416 (4%)]\tLoss: 1.010459\tSamples/sec: 4778.5\n",
            "Train Epoch: 2 [5632/60416 (8%)]\tLoss: 0.940168\tSamples/sec: 6558.0\n",
            "Train Epoch: 2 [8192/60416 (13%)]\tLoss: 0.928084\tSamples/sec: 7097.1\n",
            "Train Epoch: 2 [10752/60416 (17%)]\tLoss: 0.905592\tSamples/sec: 7390.8\n",
            "Train Epoch: 2 [13312/60416 (21%)]\tLoss: 0.827681\tSamples/sec: 7768.3\n",
            "Train Epoch: 2 [15872/60416 (25%)]\tLoss: 0.801553\tSamples/sec: 7587.7\n",
            "Train Epoch: 2 [18432/60416 (30%)]\tLoss: 0.725807\tSamples/sec: 7665.3\n",
            "Train Epoch: 2 [20992/60416 (34%)]\tLoss: 0.729663\tSamples/sec: 7825.8\n",
            "Train Epoch: 2 [23552/60416 (38%)]\tLoss: 0.688415\tSamples/sec: 7856.8\n",
            "Train Epoch: 2 [26112/60416 (42%)]\tLoss: 0.635808\tSamples/sec: 7965.6\n",
            "Train Epoch: 2 [28672/60416 (47%)]\tLoss: 0.652585\tSamples/sec: 7900.2\n",
            "Train Epoch: 2 [31232/60416 (51%)]\tLoss: 0.608215\tSamples/sec: 8047.6\n",
            "Train Epoch: 2 [33792/60416 (55%)]\tLoss: 0.590328\tSamples/sec: 8170.9\n",
            "Train Epoch: 2 [36352/60416 (59%)]\tLoss: 0.537393\tSamples/sec: 8121.9\n",
            "Train Epoch: 2 [38912/60416 (64%)]\tLoss: 0.539609\tSamples/sec: 8101.8\n",
            "Train Epoch: 2 [41472/60416 (68%)]\tLoss: 0.515938\tSamples/sec: 8138.8\n",
            "Train Epoch: 2 [44032/60416 (72%)]\tLoss: 0.528957\tSamples/sec: 8239.8\n",
            "Train Epoch: 2 [46592/60416 (76%)]\tLoss: 0.462877\tSamples/sec: 8123.4\n",
            "Train Epoch: 2 [49152/60416 (81%)]\tLoss: 0.426361\tSamples/sec: 8180.6\n",
            "Train Epoch: 2 [51712/60416 (85%)]\tLoss: 0.432524\tSamples/sec: 8279.6\n",
            "Train Epoch: 2 [54272/60416 (89%)]\tLoss: 0.415959\tSamples/sec: 8104.7\n",
            "Train Epoch: 2 [56832/60416 (93%)]\tLoss: 0.443523\tSamples/sec: 8044.8\n",
            "Train Epoch: 2 [59392/60416 (97%)]\tLoss: 0.400392\tSamples/sec: 8135.2\n",
            "\n",
            "Test set: Average loss: 0.0008, Accuracy: 8958/9728 (92.08%), Samples/sec: 8365.1\n",
            "\n",
            "Train Epoch: 3 [512/60416 (0%)]\tLoss: 0.377839\tSamples/sec: 1433.2\n",
            "Train Epoch: 3 [3072/60416 (4%)]\tLoss: 0.376003\tSamples/sec: 5088.6\n",
            "Train Epoch: 3 [5632/60416 (8%)]\tLoss: 0.383656\tSamples/sec: 6184.1\n",
            "Train Epoch: 3 [8192/60416 (13%)]\tLoss: 0.376921\tSamples/sec: 6813.9\n",
            "Train Epoch: 3 [10752/60416 (17%)]\tLoss: 0.395940\tSamples/sec: 7307.7\n",
            "Train Epoch: 3 [13312/60416 (21%)]\tLoss: 0.383109\tSamples/sec: 7607.0\n",
            "Train Epoch: 3 [15872/60416 (25%)]\tLoss: 0.343937\tSamples/sec: 7612.6\n",
            "Train Epoch: 3 [18432/60416 (30%)]\tLoss: 0.327986\tSamples/sec: 7768.1\n",
            "Train Epoch: 3 [20992/60416 (34%)]\tLoss: 0.327798\tSamples/sec: 7747.0\n",
            "Train Epoch: 3 [23552/60416 (38%)]\tLoss: 0.367912\tSamples/sec: 7950.9\n",
            "Train Epoch: 3 [26112/60416 (42%)]\tLoss: 0.347213\tSamples/sec: 7909.7\n",
            "Train Epoch: 3 [28672/60416 (47%)]\tLoss: 0.322219\tSamples/sec: 7989.5\n",
            "Train Epoch: 3 [31232/60416 (51%)]\tLoss: 0.290456\tSamples/sec: 7970.2\n",
            "Train Epoch: 3 [33792/60416 (55%)]\tLoss: 0.324023\tSamples/sec: 8082.5\n",
            "Train Epoch: 3 [36352/60416 (59%)]\tLoss: 0.313742\tSamples/sec: 8051.1\n",
            "Train Epoch: 3 [38912/60416 (64%)]\tLoss: 0.269325\tSamples/sec: 8138.6\n",
            "Train Epoch: 3 [41472/60416 (68%)]\tLoss: 0.289177\tSamples/sec: 8217.1\n",
            "Train Epoch: 3 [44032/60416 (72%)]\tLoss: 0.246960\tSamples/sec: 8107.2\n",
            "Train Epoch: 3 [46592/60416 (76%)]\tLoss: 0.263364\tSamples/sec: 8084.6\n",
            "Train Epoch: 3 [49152/60416 (81%)]\tLoss: 0.240872\tSamples/sec: 8220.9\n",
            "Train Epoch: 3 [51712/60416 (85%)]\tLoss: 0.278214\tSamples/sec: 8256.4\n",
            "Train Epoch: 3 [54272/60416 (89%)]\tLoss: 0.235321\tSamples/sec: 7979.5\n",
            "Train Epoch: 3 [56832/60416 (93%)]\tLoss: 0.260743\tSamples/sec: 8043.0\n",
            "Train Epoch: 3 [59392/60416 (97%)]\tLoss: 0.244226\tSamples/sec: 8080.4\n",
            "\n",
            "Test set: Average loss: 0.0004, Accuracy: 9225/9728 (94.83%), Samples/sec: 8260.0\n",
            "\n",
            "Train Epoch: 4 [512/60416 (0%)]\tLoss: 0.255706\tSamples/sec: 1357.3\n",
            "Train Epoch: 4 [3072/60416 (4%)]\tLoss: 0.252883\tSamples/sec: 4984.6\n",
            "Train Epoch: 4 [5632/60416 (8%)]\tLoss: 0.261299\tSamples/sec: 6353.1\n",
            "Train Epoch: 4 [8192/60416 (13%)]\tLoss: 0.217248\tSamples/sec: 7094.6\n",
            "Train Epoch: 4 [10752/60416 (17%)]\tLoss: 0.235459\tSamples/sec: 7041.6\n",
            "Train Epoch: 4 [13312/60416 (21%)]\tLoss: 0.236180\tSamples/sec: 7405.5\n",
            "Train Epoch: 4 [15872/60416 (25%)]\tLoss: 0.185465\tSamples/sec: 7595.9\n",
            "Train Epoch: 4 [18432/60416 (30%)]\tLoss: 0.236172\tSamples/sec: 7758.1\n",
            "Train Epoch: 4 [20992/60416 (34%)]\tLoss: 0.191471\tSamples/sec: 7747.9\n",
            "Train Epoch: 4 [23552/60416 (38%)]\tLoss: 0.264090\tSamples/sec: 7883.9\n",
            "Train Epoch: 4 [26112/60416 (42%)]\tLoss: 0.212188\tSamples/sec: 7972.9\n",
            "Train Epoch: 4 [28672/60416 (47%)]\tLoss: 0.178723\tSamples/sec: 8090.0\n",
            "Train Epoch: 4 [31232/60416 (51%)]\tLoss: 0.179659\tSamples/sec: 7983.2\n",
            "Train Epoch: 4 [33792/60416 (55%)]\tLoss: 0.197050\tSamples/sec: 8062.0\n",
            "Train Epoch: 4 [36352/60416 (59%)]\tLoss: 0.232272\tSamples/sec: 8169.7\n",
            "Train Epoch: 4 [38912/60416 (64%)]\tLoss: 0.225527\tSamples/sec: 8227.8\n",
            "Train Epoch: 4 [41472/60416 (68%)]\tLoss: 0.164801\tSamples/sec: 8187.0\n",
            "Train Epoch: 4 [44032/60416 (72%)]\tLoss: 0.181013\tSamples/sec: 8252.7\n",
            "Train Epoch: 4 [46592/60416 (76%)]\tLoss: 0.227063\tSamples/sec: 8329.6\n",
            "Train Epoch: 4 [49152/60416 (81%)]\tLoss: 0.171316\tSamples/sec: 8392.9\n",
            "Train Epoch: 4 [51712/60416 (85%)]\tLoss: 0.207166\tSamples/sec: 8332.7\n",
            "Train Epoch: 4 [54272/60416 (89%)]\tLoss: 0.150534\tSamples/sec: 8059.3\n",
            "Train Epoch: 4 [56832/60416 (93%)]\tLoss: 0.186073\tSamples/sec: 8116.6\n",
            "Train Epoch: 4 [59392/60416 (97%)]\tLoss: 0.170278\tSamples/sec: 8200.9\n",
            "\n",
            "Test set: Average loss: 0.0003, Accuracy: 9331/9728 (95.92%), Samples/sec: 8331.2\n",
            "\n",
            "Train Epoch: 5 [512/60416 (0%)]\tLoss: 0.165209\tSamples/sec: 1353.4\n",
            "Train Epoch: 5 [3072/60416 (4%)]\tLoss: 0.179374\tSamples/sec: 4529.1\n",
            "Train Epoch: 5 [5632/60416 (8%)]\tLoss: 0.191040\tSamples/sec: 5819.7\n",
            "Train Epoch: 5 [8192/60416 (13%)]\tLoss: 0.160953\tSamples/sec: 6444.1\n",
            "Train Epoch: 5 [10752/60416 (17%)]\tLoss: 0.167342\tSamples/sec: 6778.9\n",
            "Train Epoch: 5 [13312/60416 (21%)]\tLoss: 0.166443\tSamples/sec: 7197.8\n",
            "Train Epoch: 5 [15872/60416 (25%)]\tLoss: 0.146924\tSamples/sec: 7521.7\n",
            "Train Epoch: 5 [18432/60416 (30%)]\tLoss: 0.177641\tSamples/sec: 7705.1\n",
            "Train Epoch: 5 [20992/60416 (34%)]\tLoss: 0.186600\tSamples/sec: 7707.9\n",
            "Train Epoch: 5 [23552/60416 (38%)]\tLoss: 0.144446\tSamples/sec: 7906.8\n",
            "Train Epoch: 5 [26112/60416 (42%)]\tLoss: 0.193619\tSamples/sec: 8027.3\n",
            "Train Epoch: 5 [28672/60416 (47%)]\tLoss: 0.188367\tSamples/sec: 8006.4\n",
            "Train Epoch: 5 [31232/60416 (51%)]\tLoss: 0.169802\tSamples/sec: 8060.3\n",
            "Train Epoch: 5 [33792/60416 (55%)]\tLoss: 0.165957\tSamples/sec: 8068.2\n",
            "Train Epoch: 5 [36352/60416 (59%)]\tLoss: 0.133278\tSamples/sec: 8085.0\n",
            "Train Epoch: 5 [38912/60416 (64%)]\tLoss: 0.145122\tSamples/sec: 8123.8\n",
            "Train Epoch: 5 [41472/60416 (68%)]\tLoss: 0.182769\tSamples/sec: 8114.1\n",
            "Train Epoch: 5 [44032/60416 (72%)]\tLoss: 0.154814\tSamples/sec: 8053.3\n",
            "Train Epoch: 5 [46592/60416 (76%)]\tLoss: 0.140945\tSamples/sec: 8096.7\n",
            "Train Epoch: 5 [49152/60416 (81%)]\tLoss: 0.177001\tSamples/sec: 8083.6\n",
            "Train Epoch: 5 [51712/60416 (85%)]\tLoss: 0.168301\tSamples/sec: 8142.7\n",
            "Train Epoch: 5 [54272/60416 (89%)]\tLoss: 0.158758\tSamples/sec: 7956.5\n",
            "Train Epoch: 5 [56832/60416 (93%)]\tLoss: 0.143004\tSamples/sec: 8039.2\n",
            "Train Epoch: 5 [59392/60416 (97%)]\tLoss: 0.174266\tSamples/sec: 8080.4\n",
            "\n",
            "Test set: Average loss: 0.0003, Accuracy: 9396/9728 (96.59%), Samples/sec: 8272.5\n",
            "\n",
            "Train Epoch: 6 [512/60416 (0%)]\tLoss: 0.194558\tSamples/sec: 1311.2\n",
            "Train Epoch: 6 [3072/60416 (4%)]\tLoss: 0.133618\tSamples/sec: 4791.8\n",
            "Train Epoch: 6 [5632/60416 (8%)]\tLoss: 0.164974\tSamples/sec: 6175.5\n",
            "Train Epoch: 6 [8192/60416 (13%)]\tLoss: 0.141933\tSamples/sec: 6365.5\n",
            "Train Epoch: 6 [10752/60416 (17%)]\tLoss: 0.105338\tSamples/sec: 7137.5\n",
            "Train Epoch: 6 [13312/60416 (21%)]\tLoss: 0.112463\tSamples/sec: 7484.1\n",
            "Train Epoch: 6 [15872/60416 (25%)]\tLoss: 0.141697\tSamples/sec: 7805.6\n",
            "Train Epoch: 6 [18432/60416 (30%)]\tLoss: 0.125966\tSamples/sec: 7860.1\n",
            "Train Epoch: 6 [20992/60416 (34%)]\tLoss: 0.125652\tSamples/sec: 7824.2\n",
            "Train Epoch: 6 [23552/60416 (38%)]\tLoss: 0.143783\tSamples/sec: 7902.1\n",
            "Train Epoch: 6 [26112/60416 (42%)]\tLoss: 0.124122\tSamples/sec: 7999.4\n",
            "Train Epoch: 6 [28672/60416 (47%)]\tLoss: 0.162593\tSamples/sec: 8071.3\n",
            "Train Epoch: 6 [31232/60416 (51%)]\tLoss: 0.133403\tSamples/sec: 8112.0\n",
            "Train Epoch: 6 [33792/60416 (55%)]\tLoss: 0.113870\tSamples/sec: 8036.7\n",
            "Train Epoch: 6 [36352/60416 (59%)]\tLoss: 0.155340\tSamples/sec: 8106.0\n",
            "Train Epoch: 6 [38912/60416 (64%)]\tLoss: 0.166278\tSamples/sec: 8170.9\n",
            "Train Epoch: 6 [41472/60416 (68%)]\tLoss: 0.169740\tSamples/sec: 8197.4\n",
            "Train Epoch: 6 [44032/60416 (72%)]\tLoss: 0.193992\tSamples/sec: 8128.3\n",
            "Train Epoch: 6 [46592/60416 (76%)]\tLoss: 0.146185\tSamples/sec: 8159.0\n",
            "Train Epoch: 6 [49152/60416 (81%)]\tLoss: 0.105207\tSamples/sec: 8230.1\n",
            "Train Epoch: 6 [51712/60416 (85%)]\tLoss: 0.170491\tSamples/sec: 8247.7\n",
            "Train Epoch: 6 [54272/60416 (89%)]\tLoss: 0.113637\tSamples/sec: 7900.7\n",
            "Train Epoch: 6 [56832/60416 (93%)]\tLoss: 0.140157\tSamples/sec: 7950.6\n",
            "Train Epoch: 6 [59392/60416 (97%)]\tLoss: 0.137308\tSamples/sec: 8105.5\n",
            "\n",
            "Test set: Average loss: 0.0002, Accuracy: 9429/9728 (96.93%), Samples/sec: 8472.5\n",
            "\n",
            "Train Epoch: 7 [512/60416 (0%)]\tLoss: 0.152692\tSamples/sec: 1462.6\n",
            "Train Epoch: 7 [3072/60416 (4%)]\tLoss: 0.143911\tSamples/sec: 4988.7\n",
            "Train Epoch: 7 [5632/60416 (8%)]\tLoss: 0.114239\tSamples/sec: 6083.5\n",
            "Train Epoch: 7 [8192/60416 (13%)]\tLoss: 0.128745\tSamples/sec: 6982.8\n",
            "Train Epoch: 7 [10752/60416 (17%)]\tLoss: 0.091123\tSamples/sec: 6949.9\n",
            "Train Epoch: 7 [13312/60416 (21%)]\tLoss: 0.120432\tSamples/sec: 7333.7\n",
            "Train Epoch: 7 [15872/60416 (25%)]\tLoss: 0.114749\tSamples/sec: 7532.4\n",
            "Train Epoch: 7 [18432/60416 (30%)]\tLoss: 0.144059\tSamples/sec: 7620.1\n",
            "Train Epoch: 7 [20992/60416 (34%)]\tLoss: 0.107130\tSamples/sec: 7554.5\n",
            "Train Epoch: 7 [23552/60416 (38%)]\tLoss: 0.117540\tSamples/sec: 7644.6\n",
            "Train Epoch: 7 [26112/60416 (42%)]\tLoss: 0.108488\tSamples/sec: 7890.7\n",
            "Train Epoch: 7 [28672/60416 (47%)]\tLoss: 0.082260\tSamples/sec: 7972.3\n",
            "Train Epoch: 7 [31232/60416 (51%)]\tLoss: 0.153568\tSamples/sec: 7923.6\n",
            "Train Epoch: 7 [33792/60416 (55%)]\tLoss: 0.120512\tSamples/sec: 7999.7\n",
            "Train Epoch: 7 [36352/60416 (59%)]\tLoss: 0.149102\tSamples/sec: 8009.6\n",
            "Train Epoch: 7 [38912/60416 (64%)]\tLoss: 0.139068\tSamples/sec: 8061.8\n",
            "Train Epoch: 7 [41472/60416 (68%)]\tLoss: 0.154296\tSamples/sec: 8151.1\n",
            "Train Epoch: 7 [44032/60416 (72%)]\tLoss: 0.103651\tSamples/sec: 8151.3\n",
            "Train Epoch: 7 [46592/60416 (76%)]\tLoss: 0.109344\tSamples/sec: 8104.1\n",
            "Train Epoch: 7 [49152/60416 (81%)]\tLoss: 0.128108\tSamples/sec: 8125.6\n",
            "Train Epoch: 7 [51712/60416 (85%)]\tLoss: 0.093941\tSamples/sec: 8235.9\n",
            "Train Epoch: 7 [54272/60416 (89%)]\tLoss: 0.116608\tSamples/sec: 8011.9\n",
            "Train Epoch: 7 [56832/60416 (93%)]\tLoss: 0.112303\tSamples/sec: 8000.0\n",
            "Train Epoch: 7 [59392/60416 (97%)]\tLoss: 0.133280\tSamples/sec: 8045.1\n",
            "\n",
            "Test set: Average loss: 0.0002, Accuracy: 9463/9728 (97.28%), Samples/sec: 8186.4\n",
            "\n",
            "Train Epoch: 8 [512/60416 (0%)]\tLoss: 0.131306\tSamples/sec: 1263.9\n",
            "Train Epoch: 8 [3072/60416 (4%)]\tLoss: 0.158764\tSamples/sec: 4836.2\n",
            "Train Epoch: 8 [5632/60416 (8%)]\tLoss: 0.125909\tSamples/sec: 6089.6\n",
            "Train Epoch: 8 [8192/60416 (13%)]\tLoss: 0.147395\tSamples/sec: 6912.0\n",
            "Train Epoch: 8 [10752/60416 (17%)]\tLoss: 0.110592\tSamples/sec: 6997.1\n",
            "Train Epoch: 8 [13312/60416 (21%)]\tLoss: 0.101210\tSamples/sec: 7444.9\n",
            "Train Epoch: 8 [15872/60416 (25%)]\tLoss: 0.113973\tSamples/sec: 7473.1\n",
            "Train Epoch: 8 [18432/60416 (30%)]\tLoss: 0.099423\tSamples/sec: 7638.2\n",
            "Train Epoch: 8 [20992/60416 (34%)]\tLoss: 0.102155\tSamples/sec: 7747.6\n",
            "Train Epoch: 8 [23552/60416 (38%)]\tLoss: 0.112744\tSamples/sec: 7806.0\n",
            "Train Epoch: 8 [26112/60416 (42%)]\tLoss: 0.105272\tSamples/sec: 7780.8\n",
            "Train Epoch: 8 [28672/60416 (47%)]\tLoss: 0.142407\tSamples/sec: 7868.4\n",
            "Train Epoch: 8 [31232/60416 (51%)]\tLoss: 0.069700\tSamples/sec: 8000.2\n",
            "Train Epoch: 8 [33792/60416 (55%)]\tLoss: 0.111519\tSamples/sec: 7965.7\n",
            "Train Epoch: 8 [36352/60416 (59%)]\tLoss: 0.097003\tSamples/sec: 7968.5\n",
            "Train Epoch: 8 [38912/60416 (64%)]\tLoss: 0.119389\tSamples/sec: 8123.7\n",
            "Train Epoch: 8 [41472/60416 (68%)]\tLoss: 0.104785\tSamples/sec: 8203.1\n",
            "Train Epoch: 8 [44032/60416 (72%)]\tLoss: 0.092431\tSamples/sec: 8265.9\n",
            "Train Epoch: 8 [46592/60416 (76%)]\tLoss: 0.113900\tSamples/sec: 8238.1\n",
            "Train Epoch: 8 [49152/60416 (81%)]\tLoss: 0.125788\tSamples/sec: 8292.7\n",
            "Train Epoch: 8 [51712/60416 (85%)]\tLoss: 0.128149\tSamples/sec: 8314.4\n",
            "Train Epoch: 8 [54272/60416 (89%)]\tLoss: 0.126405\tSamples/sec: 8037.6\n",
            "Train Epoch: 8 [56832/60416 (93%)]\tLoss: 0.097243\tSamples/sec: 8050.7\n",
            "Train Epoch: 8 [59392/60416 (97%)]\tLoss: 0.100696\tSamples/sec: 8105.0\n",
            "\n",
            "Test set: Average loss: 0.0002, Accuracy: 9480/9728 (97.45%), Samples/sec: 8407.0\n",
            "\n",
            "Train Epoch: 9 [512/60416 (0%)]\tLoss: 0.117445\tSamples/sec: 1502.5\n",
            "Train Epoch: 9 [3072/60416 (4%)]\tLoss: 0.097885\tSamples/sec: 5210.0\n",
            "Train Epoch: 9 [5632/60416 (8%)]\tLoss: 0.091771\tSamples/sec: 6422.3\n",
            "Train Epoch: 9 [8192/60416 (13%)]\tLoss: 0.112436\tSamples/sec: 7014.4\n",
            "Train Epoch: 9 [10752/60416 (17%)]\tLoss: 0.088707\tSamples/sec: 6985.5\n",
            "Train Epoch: 9 [13312/60416 (21%)]\tLoss: 0.094637\tSamples/sec: 7297.4\n",
            "Train Epoch: 9 [15872/60416 (25%)]\tLoss: 0.099227\tSamples/sec: 7376.5\n",
            "Train Epoch: 9 [18432/60416 (30%)]\tLoss: 0.090781\tSamples/sec: 7628.5\n",
            "Train Epoch: 9 [20992/60416 (34%)]\tLoss: 0.122216\tSamples/sec: 7557.9\n",
            "Train Epoch: 9 [23552/60416 (38%)]\tLoss: 0.100741\tSamples/sec: 7681.1\n",
            "Train Epoch: 9 [26112/60416 (42%)]\tLoss: 0.071576\tSamples/sec: 7720.6\n",
            "Train Epoch: 9 [28672/60416 (47%)]\tLoss: 0.113011\tSamples/sec: 7906.1\n",
            "Train Epoch: 9 [31232/60416 (51%)]\tLoss: 0.105612\tSamples/sec: 7786.1\n",
            "Train Epoch: 9 [33792/60416 (55%)]\tLoss: 0.094170\tSamples/sec: 7909.8\n",
            "Train Epoch: 9 [36352/60416 (59%)]\tLoss: 0.112324\tSamples/sec: 8069.0\n",
            "Train Epoch: 9 [38912/60416 (64%)]\tLoss: 0.082112\tSamples/sec: 8137.3\n",
            "Train Epoch: 9 [41472/60416 (68%)]\tLoss: 0.128701\tSamples/sec: 8153.7\n",
            "Train Epoch: 9 [44032/60416 (72%)]\tLoss: 0.098595\tSamples/sec: 8249.9\n",
            "Train Epoch: 9 [46592/60416 (76%)]\tLoss: 0.090200\tSamples/sec: 8122.2\n",
            "Train Epoch: 9 [49152/60416 (81%)]\tLoss: 0.093385\tSamples/sec: 8172.5\n",
            "Train Epoch: 9 [51712/60416 (85%)]\tLoss: 0.084971\tSamples/sec: 8242.5\n",
            "Train Epoch: 9 [54272/60416 (89%)]\tLoss: 0.090854\tSamples/sec: 8129.1\n",
            "Train Epoch: 9 [56832/60416 (93%)]\tLoss: 0.082898\tSamples/sec: 7894.5\n",
            "Train Epoch: 9 [59392/60416 (97%)]\tLoss: 0.074912\tSamples/sec: 8039.5\n",
            "\n",
            "Test set: Average loss: 0.0002, Accuracy: 9495/9728 (97.60%), Samples/sec: 8410.7\n",
            "\n",
            "Train Epoch: 10 [512/60416 (0%)]\tLoss: 0.085305\tSamples/sec: 1415.2\n",
            "Train Epoch: 10 [3072/60416 (4%)]\tLoss: 0.091786\tSamples/sec: 5199.9\n",
            "Train Epoch: 10 [5632/60416 (8%)]\tLoss: 0.072622\tSamples/sec: 6309.7\n",
            "Train Epoch: 10 [8192/60416 (13%)]\tLoss: 0.106193\tSamples/sec: 7040.7\n",
            "Train Epoch: 10 [10752/60416 (17%)]\tLoss: 0.076992\tSamples/sec: 7280.6\n",
            "Train Epoch: 10 [13312/60416 (21%)]\tLoss: 0.094210\tSamples/sec: 7433.2\n",
            "Train Epoch: 10 [15872/60416 (25%)]\tLoss: 0.095921\tSamples/sec: 7388.3\n",
            "Train Epoch: 10 [18432/60416 (30%)]\tLoss: 0.094609\tSamples/sec: 7639.0\n",
            "Train Epoch: 10 [20992/60416 (34%)]\tLoss: 0.068337\tSamples/sec: 7842.8\n",
            "Train Epoch: 10 [23552/60416 (38%)]\tLoss: 0.087292\tSamples/sec: 7806.9\n",
            "Train Epoch: 10 [26112/60416 (42%)]\tLoss: 0.071750\tSamples/sec: 7865.6\n",
            "Train Epoch: 10 [28672/60416 (47%)]\tLoss: 0.092966\tSamples/sec: 7737.4\n",
            "Train Epoch: 10 [31232/60416 (51%)]\tLoss: 0.093082\tSamples/sec: 7896.9\n",
            "Train Epoch: 10 [33792/60416 (55%)]\tLoss: 0.092716\tSamples/sec: 8000.6\n",
            "Train Epoch: 10 [36352/60416 (59%)]\tLoss: 0.092672\tSamples/sec: 7976.2\n",
            "Train Epoch: 10 [38912/60416 (64%)]\tLoss: 0.080715\tSamples/sec: 8006.2\n",
            "Train Epoch: 10 [41472/60416 (68%)]\tLoss: 0.108182\tSamples/sec: 8033.4\n",
            "Train Epoch: 10 [44032/60416 (72%)]\tLoss: 0.103526\tSamples/sec: 8113.7\n",
            "Train Epoch: 10 [46592/60416 (76%)]\tLoss: 0.084049\tSamples/sec: 8128.3\n",
            "Train Epoch: 10 [49152/60416 (81%)]\tLoss: 0.072251\tSamples/sec: 8085.3\n",
            "Train Epoch: 10 [51712/60416 (85%)]\tLoss: 0.101098\tSamples/sec: 8156.4\n",
            "Train Epoch: 10 [54272/60416 (89%)]\tLoss: 0.075420\tSamples/sec: 7906.3\n",
            "Train Epoch: 10 [56832/60416 (93%)]\tLoss: 0.079525\tSamples/sec: 7972.8\n",
            "Train Epoch: 10 [59392/60416 (97%)]\tLoss: 0.075124\tSamples/sec: 8017.2\n",
            "\n",
            "Test set: Average loss: 0.0002, Accuracy: 9509/9728 (97.75%), Samples/sec: 8097.9\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "97.74876644736842"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    }
  ]
}