# TODO: Try removing with the next pin update. See https://github.com/pytorch/xla/pull/4615#issuecomment-1428883781
diff --git a/xla/debug_options_flags.cc b/xla/debug_options_flags.cc
index 07eaf1e7f..0dbeec28f 100644
--- a/xla/debug_options_flags.cc
+++ b/xla/debug_options_flags.cc
@@ -111,7 +111,6 @@ DebugOptions DefaultDebugOptionsIgnoringFlags() {
       DebugOptions::PARTITIONING_ALGORITHM_NOOP);
 
   opts.set_xla_gpu_enable_triton_gemm(false);
-  opts.set_xla_gpu_enable_cudnn_int8x32_convolution_reordering(false);
   opts.set_xla_gpu_triton_gemm_any(false);
   return opts;
 }
@@ -890,13 +889,6 @@ void MakeDebugOptionsFlags(std::vector<tsl::Flag>* flag_list,
                 bool_setter_for(&DebugOptions::set_xla_gpu_enable_triton_gemm),
                 debug_options->xla_gpu_enable_triton_gemm(),
                 "Use Triton-based matrix multiplication."));
-  flag_list->push_back(tsl::Flag(
-      "xla_gpu_enable_cudnn_int8x32_convolution_reordering",
-      bool_setter_for(
-          &DebugOptions::
-              set_xla_gpu_enable_cudnn_int8x32_convolution_reordering),
-      debug_options->xla_gpu_enable_cudnn_int8x32_convolution_reordering(),
-      "Enable cuDNN frontend for int8x32 convolutions with reordered filter."));
   flag_list->push_back(
       tsl::Flag("xla_gpu_triton_gemm_any",
                 bool_setter_for(&DebugOptions::set_xla_gpu_triton_gemm_any),
diff --git a/xla/service/gpu/BUILD b/xla/service/gpu/BUILD
index ab283df9b..5cc5171ee 100644
--- a/xla/service/gpu/BUILD
+++ b/xla/service/gpu/BUILD
@@ -1663,11 +1663,8 @@ cc_library(
     srcs = ["cudnn_vectorize_convolutions.cc"],
     hdrs = ["cudnn_vectorize_convolutions.h"],
     deps = [
-        ":backend_configs_cc",
-        ":cublas_cudnn",
         ":cudnn_support_utils",
         ":stream_executor_util",
-        "//xla:shape_util",
         "//xla:statusor",
         "//xla/client:xla_builder",
         "//xla/hlo/ir:hlo",
diff --git a/xla/service/gpu/conv_layout_normalization.cc b/xla/service/gpu/conv_layout_normalization.cc
index b6f7a28d3..2e2aa59a8 100644
--- a/xla/service/gpu/conv_layout_normalization.cc
+++ b/xla/service/gpu/conv_layout_normalization.cc
@@ -16,24 +16,23 @@ limitations under the License.
 #include "xla/service/gpu/conv_layout_normalization.h"
 
 #include <optional>
-#include <tuple>
 #include <vector>
 
 #include "xla/hlo/ir/hlo_casting_utils.h"
 #include "xla/hlo/ir/hlo_instruction.h"
-#include "xla/hlo/ir/hlo_instructions.h"
 #include "xla/hlo/ir/hlo_module.h"
-#include "xla/layout_util.h"
 #include "xla/service/gpu/cublas_cudnn.h"
 #include "xla/service/hlo_creation_utils.h"
-#include "xla/shape_util.h"
 
 namespace xla {
 namespace gpu {
-namespace {
 
-StatusOr<HloInstruction*> UpdateLayoutForCudnnConvolution(
-    HloCustomCallInstruction* hlo) {
+StatusOr<std::optional<HloInstruction*>>
+NormalizeLayoutForCustomCallConvolution(HloCustomCallInstruction* hlo) {
+  if (!IsCustomCallToDnnConvolution(*hlo)) {
+    return {std::nullopt};
+  }
+
   HloInstruction* lhs = hlo->mutable_operand(0);
   HloInstruction* rhs = hlo->mutable_operand(1);
   const ConvolutionDimensionNumbers& dim_numbers =
@@ -163,7 +162,7 @@ StatusOr<HloInstruction*> UpdateLayoutForCudnnConvolution(
   } else {
     bc_to_orig = MakeBitcastHlo(normalized_conv, hlo->shape());
   }
-  return bc_to_orig;
+  return std::make_optional(bc_to_orig);
 }
 
 // Create an instruction sequence (reshape-transpose-reshape) that effectively
@@ -217,61 +216,5 @@ HloInstruction* CreateTransposeForCudnnBiasReordering(HloInstruction* hlo,
   return bitcast_2;
 }
 
-// Normalize the layout of cuDNN int8x32 filter reordering custom call
-// (implemented by calling `cudnnReorderFilterAndBias`), which should be
-// followed by a convolution.
-// Both the input and the output shape for the filter operand must have the
-// NCHW_VECT_C layout.
-HloInstruction* UpdateLayoutForCudnnConvolutionReordering(
-    HloCustomCallInstruction* hlo) {
-  // The custom call may have either one (filter) or two (filter and bias)
-  // operands. The number of outputs matches the number of inputs.
-  Shape const* filter_shape;
-  Shape const* bias_shape;
-  std::tie(filter_shape, bias_shape) =
-      hlo->shape().IsTuple() ? std::make_tuple(&hlo->shape().tuple_shapes(0),
-                                               &hlo->shape().tuple_shapes(1))
-                             : std::make_tuple(&hlo->shape(), nullptr);
-
-  // Transpose the filter to match the expected layout (NCHW_VECT_C).
-  // This bias is 1D, so the shape doesn't need to be updated.
-  auto new_filter_shape =
-      ShapeUtil::MakeShapeWithDescendingLayoutAndSamePhysicalLayout(
-          *filter_shape);
-  auto dimensions = LayoutUtil::MakeLayoutFromMajorToMinor(
-      filter_shape->layout().minor_to_major());
-  HloInstruction* transpose = hlo->AddInstruction(
-      HloInstruction::CreateTranspose(new_filter_shape, hlo->mutable_operand(0),
-                                      dimensions.minor_to_major()));
-
-  // Create a replacement custom-call with layout-normalized inputs.
-  HloInstruction* result;
-  if (bias_shape != nullptr) {
-    result = MaybeMakeTuple(
-        {CreateTransposeForCudnnFilterReordering(transpose, new_filter_shape),
-         CreateTransposeForCudnnBiasReordering(hlo->mutable_operand(1),
-                                               *bias_shape)});
-  } else {
-    result =
-        CreateTransposeForCudnnFilterReordering(transpose, new_filter_shape);
-  }
-  return MakeBitcastHlo(result, hlo->shape());
-}
-
-}  // namespace
-
-StatusOr<std::optional<HloInstruction*>> NormalizeLayoutForGpuCustomCalls(
-    HloCustomCallInstruction* hlo) {
-  if (IsCustomCallToDnnConvolution(*hlo)) {
-    TF_ASSIGN_OR_RETURN(HloInstruction * bc_to_orig,
-                        UpdateLayoutForCudnnConvolution(hlo));
-    return std::make_optional(bc_to_orig);
-  }
-  if (IsCudnnConvolutionReorder(*hlo)) {
-    return std::make_optional(UpdateLayoutForCudnnConvolutionReordering(hlo));
-  }
-  return {std::nullopt};
-}
-
 }  // end namespace gpu
 }  // end namespace xla
diff --git a/xla/service/gpu/conv_layout_normalization.h b/xla/service/gpu/conv_layout_normalization.h
index 0199bddaf..009307539 100644
--- a/xla/service/gpu/conv_layout_normalization.h
+++ b/xla/service/gpu/conv_layout_normalization.h
@@ -28,8 +28,8 @@ limitations under the License.
 namespace xla {
 namespace gpu {
 
-StatusOr<std::optional<HloInstruction*>> NormalizeLayoutForGpuCustomCalls(
-    HloCustomCallInstruction*);
+StatusOr<std::optional<HloInstruction*>>
+NormalizeLayoutForCustomCallConvolution(HloCustomCallInstruction*);
 
 }  // end namespace gpu
 }  // end namespace xla
diff --git a/xla/service/gpu/cudnn_vectorize_convolutions.cc b/xla/service/gpu/cudnn_vectorize_convolutions.cc
index c9b8c3255..c1d3db89a 100644
--- a/xla/service/gpu/cudnn_vectorize_convolutions.cc
+++ b/xla/service/gpu/cudnn_vectorize_convolutions.cc
@@ -16,22 +16,17 @@ limitations under the License.
 #include "xla/service/gpu/cudnn_vectorize_convolutions.h"
 
 #include <optional>
-#include <string>
 #include <tuple>
 #include <vector>
 
 #include "xla/client/xla_builder.h"
 #include "xla/hlo/ir/hlo_casting_utils.h"
 #include "xla/hlo/ir/hlo_instructions.h"
-#include "xla/service/gpu/backend_configs.pb.h"
-#include "xla/service/gpu/cublas_cudnn.h"
 #include "xla/service/gpu/cudnn_support_utils.h"
 #include "xla/service/gpu/stream_executor_util.h"
-#include "xla/shape_util.h"
 
 namespace xla {
 namespace gpu {
-namespace {
 
 // Finds convolutions that this pass may be able to transform, namely int8_t
 // cudnn forward or forward-bias-activation convolutions
@@ -255,37 +250,6 @@ static ConvolutionDimensionNumbers VectorizeDnums(
   return dnums;
 }
 
-// Reorders the convolution's filter and bias (if present) according to
-// cudnnReorderFilterAndBias.  Also marks that the filter + bias are reordered
-// in the conv's backend-config.
-Status ReorderInt8NchwVect(HloCustomCallInstruction* conv, XlaOp* operands) {
-  // Update convolution backend config.
-  TF_ASSIGN_OR_RETURN(auto config,
-                      conv->backend_config<CudnnConvBackendConfig>());
-  config.set_reordered_int8_nchw_vect(true);
-  TF_RETURN_IF_ERROR(conv->set_backend_config(config));
-
-  XlaBuilder& builder = *operands->builder();
-  Shape filter_shape = builder.GetShape(operands[1]).value();
-
-  if (conv->operand_count() > 2) {
-    // Reorder filter and bias.
-    Shape bias_shape = builder.GetShape(operands[2]).value();
-    XlaOp reorder = CustomCall(
-        &builder, std::string(kCudnnConvReorderFilterAndBiasCallTarget),
-        {operands[1], operands[2]},
-        ShapeUtil::MakeTupleShape({filter_shape, bias_shape}));
-    operands[1] = GetTupleElement(reorder, 0);
-    operands[2] = GetTupleElement(reorder, 1);
-  } else {
-    // Reorder just the filter.
-    operands[1] =
-        CustomCall(&builder, std::string(kCudnnConvReorderFilterCallTarget),
-                   {operands[1]}, filter_shape);
-  }
-  return OkStatus();
-}
-
 // Tries to vectorize an already-vectorized convolution.
 //
 // That is, given a convolution of shape [N, C/k, H, W, k], changes it to have
@@ -373,17 +337,6 @@ static StatusOr<bool> TryRevectorizeConv(
         conv->ToString());
   }
 
-  // Reorder filter and bias for the int8x32 convolutions.  This requires cudnn
-  // >= 8.3.0.
-  //
-  // TODO(jlebar): Remove this guard once JAX no longer supports cudnn 8.3.
-  const auto& debug_options = conv->GetModule()->config().debug_options();
-  if (input_shape.element_type() == xla::S8 && vect_size == 32 &&
-      debug_options.xla_gpu_enable_cudnn_int8x32_convolution_reordering() &&
-      cudnn_version >= se::dnn::VersionInfo{8, 3, 0}) {
-    TF_RETURN_IF_ERROR(ReorderInt8NchwVect(conv, new_operands.data()));
-  }
-
   // The custom-call returns a tuple (new_output_shape, u8[0]), where the second
   // value in the tuple represents the convolution's scratch memory.
   DimensionVector new_output_dims(output_shape.dimensions().begin(),
@@ -509,17 +462,6 @@ static StatusOr<bool> TryVectorizeConv(
         conv->ToString());
   }
 
-  // Reorder filter and bias for the int8x32 convolutions.  This requires cudnn
-  // >= 8.3.0.
-  //
-  // TODO(jlebar): Remove this guard once JAX no longer supports cudnn 8.3.
-  const auto& debug_options = conv->GetModule()->config().debug_options();
-  if (input_shape.element_type() == xla::S8 && vect_size == 32 &&
-      debug_options.xla_gpu_enable_cudnn_int8x32_convolution_reordering() &&
-      cudnn_version >= se::dnn::VersionInfo{8, 3, 0}) {
-    TF_RETURN_IF_ERROR(ReorderInt8NchwVect(conv, new_operands.data()));
-  }
-
   // The custom-call returns a tuple (new_output_shape, u8[0]), where the second
   // value in the tuple represents the convolution's scratch memory.
   Shape new_output_shape = SplitShapeAtDim(
@@ -556,8 +498,6 @@ static StatusOr<bool> TryVectorizeConv(
   return true;
 }
 
-}  // namespace
-
 StatusOr<bool> CudnnVectorizeConvolutions::Run(
     HloModule* module,
     const absl::flat_hash_set<absl::string_view>& execution_threads) {
diff --git a/xla/service/gpu/gpu_compiler.cc b/xla/service/gpu/gpu_compiler.cc
index ffb41d325..fcd7c3105 100644
--- a/xla/service/gpu/gpu_compiler.cc
+++ b/xla/service/gpu/gpu_compiler.cc
@@ -841,7 +841,8 @@ Status GpuCompiler::OptimizeHloPostLayoutAssignment(
     pipeline.AddPass<GemmBroadcastFoldingRewriter>();
 
     if (debug_options.xla_gpu_normalize_layouts()) {
-      pipeline.AddPass<LayoutNormalization>(&NormalizeLayoutForGpuCustomCalls);
+      pipeline.AddPass<LayoutNormalization>(
+          &NormalizeLayoutForCustomCallConvolution);
       pipeline.AddPass<HloPassFix<AlgebraicSimplifier>>(options);
     }
     pipeline.AddPass<BroadcastCanonicalizer>();
diff --git a/xla/service/gpu/gpu_compiler.cc b/xla/service/gpu/gpu_compiler.cc
index ffb41d325..fcd7c3105 100644
--- a/xla/service/gpu/gpu_compiler.cc
+++ b/xla/service/gpu/gpu_compiler.cc
@@ -841,7 +841,8 @@ Status GpuCompiler::OptimizeHloPostLayoutAssignment(
     pipeline.AddPass<GemmBroadcastFoldingRewriter>();
 
     if (debug_options.xla_gpu_normalize_layouts()) {
-      pipeline.AddPass<LayoutNormalization>(&NormalizeLayoutForGpuCustomCalls);
+      pipeline.AddPass<LayoutNormalization>(
+          &NormalizeLayoutForCustomCallConvolution);
       pipeline.AddPass<HloPassFix<AlgebraicSimplifier>>(options);
     }
     pipeline.AddPass<BroadcastCanonicalizer>();
diff --git a/xla/stream_executor/cuda/cuda_dnn.cc b/xla/stream_executor/cuda/cuda_dnn.cc
index 2d9c54420..0764e16a9 100644
--- a/xla/stream_executor/cuda/cuda_dnn.cc
+++ b/xla/stream_executor/cuda/cuda_dnn.cc
@@ -3485,7 +3485,7 @@ std::tuple<int, int> GetTensorVectorSizeAndDim(
 tsl::StatusOr<cudnn_frontend::Tensor> CreateCudnnTensor(
     absl::Span<const int64_t> dims, absl::Span<const int64_t> strides,
     int64_t uid, dnn::DataType dtype, int64_t vec_count, int64_t vec_dim,
-    bool is_virtual = false, bool is_reordered_nchw_vect = false) {
+    bool is_virtual = false) {
   if (is_reordered_nchw_vect && (CUDNN_VERSION) < 8300) {
     return tsl::errors::Internal(
         "reordered nchw_vect requires cudnn 8.3+, but version was %d",
@@ -3501,10 +3501,6 @@ tsl::StatusOr<cudnn_frontend::Tensor> CreateCudnnTensor(
                     .setVirtual(is_virtual)
 // TODO(jlebar): remove guard after JAX no longer supports old cudnn
 #if CUDNN_VERSION >= 8300
-                    .setReorderType(is_reordered_nchw_vect
-
-                                        ? CUDNN_TENSOR_REORDERING_INT8x32
-                                        : CUDNN_TENSOR_REORDERING_NONE)
 #endif
                     .build();
   RETURN_MSG_IF_CUDNN_ERROR(tensor);
@@ -3532,6 +3528,11 @@ GetCudnnOperationGraph(dnn::ConvolutionKind kind, dnn::DataType input_type,
   std::vector<int64_t> input_strides = input_descriptor.vectorized_strides(
       dnn::DataLayout::kBatchDepthYX, vector_size, vector_dim);
 
+  if (vector_size == 32) {
+    return tsl::errors::Internal(
+        "cuDNN frontend doesn't support Tx32 at the moment.");
+  }
+
   TF_ASSIGN_OR_RETURN(auto tensor_x,
                       CreateCudnnTensor(input_dims, input_strides, 'x',
                                         input_type, vector_size, vector_dim));
@@ -3556,13 +3557,9 @@ GetCudnnOperationGraph(dnn::ConvolutionKind kind, dnn::DataType input_type,
   std::vector<int64_t> filter_strides = filter_descriptor.vectorized_strides(
       dnn::FilterLayout::kOutputInputYX, vector_size, vector_dim);
 
-  TF_ASSIGN_OR_RETURN(
-      auto tensor_w,
-      CreateCudnnTensor(
-          filter_dims, filter_strides, 'w', input_type, vector_size, vector_dim,
-          /*is_virtual=*/false,
-          /*is_reordered_nchw_vect=*/filter_descriptor.layout() ==
-              dnn::FilterLayout::kOutputInputYX32_CudnnReordered));
+  TF_ASSIGN_OR_RETURN(auto tensor_w,
+                      CreateCudnnTensor(filter_dims, filter_strides, 'w',
+                                        input_type, vector_size, vector_dim));
 
   // conv_desc.
   auto mode = convolution_descriptor.convolution_not_crosscorr()
@@ -3666,6 +3663,11 @@ GetCudnnFusedOperationGraph(
   std::vector<int64_t> input_strides = input_descriptor.vectorized_strides(
       dnn::DataLayout::kBatchDepthYX, vector_size, vector_dim);
 
+  if (vector_size == 32) {
+    return tsl::errors::Internal(
+        "cuDNN frontend doesn't support Tx32 at the moment.");
+  }
+
   TF_ASSIGN_OR_RETURN(auto tensor_x,
                       CreateCudnnTensor(input_dims, input_strides, 'x',
                                         input_type, vector_size, vector_dim));
@@ -3690,13 +3692,10 @@ GetCudnnFusedOperationGraph(
       dnn::FilterLayout::kOutputInputYX, vector_size, vector_dim);
   std::vector<int64_t> filter_strides = filter_descriptor.vectorized_strides(
       dnn::FilterLayout::kOutputInputYX, vector_size, vector_dim);
-  TF_ASSIGN_OR_RETURN(
-      auto tensor_w,
-      CreateCudnnTensor(
-          filter_dims, filter_strides, 'w', input_type, vector_size, vector_dim,
-          /*is_virtual=*/false,
-          /*is_reordered_nchw_vect=*/filter_descriptor.layout() ==
-              dnn::FilterLayout::kOutputInputYX32_CudnnReordered));
+
+  TF_ASSIGN_OR_RETURN(auto tensor_w,
+                      CreateCudnnTensor(filter_dims, filter_strides, 'w',
+                                        input_type, vector_size, vector_dim));
 
   // For the purposes of the cudnn graph, say that the bias tensor has the same
   // layout as the output tensor.  It doesn't actually matter, because bias is a
@@ -4831,20 +4830,17 @@ tsl::Status CudnnSupport::GetConvolveRunners(
     const dnn::ConvolutionDescriptor& convolution_descriptor, bool use_fallback,
     ScratchAllocator* /*scratch_allocator*/,
     std::vector<std::unique_ptr<const dnn::ConvRunner>>* out_exec_plans) {
+  // All current versions of the frontend API lack support for Tx32
+  // convolutions.
+  const bool is_unsupported_x32 =
+      input_descriptor.layout() == dnn::kBatchDepthYX32;
+
   // cuDNN frontend support became sufficiently stable to use in 8.1.
   // TODO(awpr): remove this condition once support for cuDNN 8.0 is dropped.
   const bool is_pre_frontend_cudnn = CUDNN_VERSION < 8100;
 
-  // cuDNN frontend support for Tx32 convolutions added in 8.3.
-  // If the filter is not reordered, do not use frontend (it is slow).
-  const bool is_disabled_x32 =
-      input_descriptor.layout() == dnn::kBatchDepthYX32 &&
-      (CUDNN_VERSION < 8300 ||
-       filter_descriptor.layout() !=
-           dnn::FilterLayout::kOutputInputYX32_CudnnReordered);
-
   const bool actually_use_cudnn_frontend =
-      use_cudnn_frontend && !is_pre_frontend_cudnn && !is_disabled_x32;
+      use_cudnn_frontend && !is_pre_frontend_cudnn && !is_unsupported_x32;
 
   if (use_cudnn_frontend && !actually_use_cudnn_frontend) {
     // This will happen once per unique conv configuration/shape that gets
@@ -4856,8 +4852,8 @@ tsl::Status CudnnSupport::GetConvolveRunners(
               << "  filter: " << filter_descriptor.ToString() << "\n"
               << "  " << convolution_descriptor.ToString() << "\n"
               << "  ... because "
-              << (is_disabled_x32
-                      ? "Tx32 convolutions are disabled."
+             << (is_unsupported_x32
+                      ? "Tx32 convolutions are unsupported."
                       : "the current cuDNN version does not support it.");
   }
 
@@ -4942,12 +4938,6 @@ CudnnSupport::ConvolveRunnerFromDesc(
         ToCudnnDataType(GetConvAccumulatorType(input_type)));
     conv.set_use_tensor_op_math(algorithm_desc.tensor_ops_enabled());
 
-    if (filter_descriptor.layout() ==
-        dnn::FilterLayout::kOutputInputYX32_CudnnReordered) {
-      CHECK_CUDNN_OK(
-          cudnnSetConvolutionReorderType(conv.handle(), CUDNN_NO_REORDER));
-    }
-
     TF_ASSIGN_OR_RETURN(
         auto runner,
         CudnnLegacyConvRunner::Create(
@@ -5209,12 +5199,6 @@ CudnnSupport::FusedConvolveRunnerFromDesc(
         ToCudnnDataType(GetConvAccumulatorType(input_type)));
     conv.set_use_tensor_op_math(algorithm_desc.tensor_ops_enabled());
 
-    if (filter_descriptor.layout() ==
-        dnn::FilterLayout::kOutputInputYX32_CudnnReordered) {
-      CHECK_CUDNN_OK(
-          cudnnSetConvolutionReorderType(conv.handle(), CUDNN_NO_REORDER));
-    }
-
     // CUDNN v6 only supports CUDNN_NOT_PROPAGATE_NAN as the reluNanOpt for
     // activation descriptor. Note that this will change the nan propagation
     // behavior from separate conv, bias, and relu (which by default is
@@ -5284,26 +5268,25 @@ tsl::Status CudnnSupport::GetFusedConvolveRunners(
       false;
 #endif
 
+  // All current versions of the frontend API lack support for Tx32
+  // convolutions.
+  const bool is_unsupported_x32 =
+      input_descriptor.layout() == dnn::kBatchDepthYX32;
+
+
   // cuDNN frontend support became sufficiently stable to use in 8.1.
   // TODO(awpr): remove this condition once support for cuDNN 8.0 is dropped.
   const bool is_pre_frontend_cudnn = CUDNN_VERSION < 8100;
 
-  // cuDNN frontend support for Tx32 convolutions added in 8.3.
-  // If the filter is not reordered, do not use frontend (it is slow).
-  const bool is_disabled_x32 =
-      input_descriptor.layout() == dnn::kBatchDepthYX32 &&
-      (CUDNN_VERSION < 8300 ||
-       filter_descriptor.layout() !=
-           dnn::FilterLayout::kOutputInputYX32_CudnnReordered);
-
   const bool actually_use_cudnn_frontend =
       use_cudnn_frontend && !is_pre_frontend_cudnn &&
-      !is_broken_identity_fused_conv && !is_disabled_x32;
+      !is_broken_identity_fused_conv && !is_unsupported_x32;
+
 
   if (use_cudnn_frontend && !actually_use_cudnn_frontend) {
     const char* reason = "the current cuDNN version does not support it.";
-    if (is_disabled_x32) {
-      reason = "Tx32 convolutions are disabled.";
+    if (is_unsupported_x32) {
+      reason = "Tx32 convolutions are unsupported.";
     } else if (is_broken_identity_fused_conv) {
       reason = "it uses an identity activation.";
     }
diff --git a/xla/tests/BUILD b/xla/tests/BUILD
index 66891709c..e6af9e14c 100644
--- a/xla/tests/BUILD
+++ b/xla/tests/BUILD
@@ -1298,7 +1298,7 @@ xla_test(
     srcs = ["convolution_cudnn_test.cc"],
     backend_tags = {"gpu": [
         "gpu",
-        "requires-gpu-sm80",
+        "requires-gpu-sm70",
     ]},
     backends = ["gpu"],
     deps = [
diff --git a/xla/tests/convolution_cudnn_test.cc b/xla/tests/convolution_cudnn_test.cc
index 22a6ee68e..81b778157 100644
--- a/xla/tests/convolution_cudnn_test.cc
+++ b/xla/tests/convolution_cudnn_test.cc
@@ -66,64 +66,6 @@ ENTRY TestComputation {
   EXPECT_TRUE(RunAndCompare(kHlo, ErrorSpec{0, 0}));
 }
 
-XLA_TEST_F(ConvolutionHloTest, TestCudnnConvInt8x32BiasNonConst) {
-  // Test two GPU compiled HLOs, first version with vectorization disabled,
-  // second with vectorization enabled. The reference implementation
-  // (Interpreter) does not support the fused conv-add-relu-clamp operation,
-  // thus cannot be used.
-  if (!backend()
-           .default_stream_executor()
-           ->GetDeviceDescription()
-           .cuda_compute_capability()
-           .IsAtLeast(8)) {
-    return;
-  }
-  constexpr char kHloBase[] = R"(
-HloModule TestModule, entry_computation_layout={(s8[4,48,48,64]{3,2,1,0},s8[64,3,3,64]{3,2,1,0},s8[64]{0})->s8[4,48,48,64]{3,2,1,0}}
-
-ENTRY TestComputation {
-  input = s8[4,48,48,64]{3,2,1,0} parameter(0)
-  filter = s8[64,3,3,64]{3,2,1,0} parameter(1)
-  bias = s8[64]{0} parameter(2)
-  convert.1 = f32[64]{0} convert(bias)
-  cudnn-conv-bias-activation.3 = (s8[4,48,48,64]{3,2,1,0}, u8[0]{0}) custom-call(input, filter, convert.1),
-      window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target="__cudnn$convBiasActivationForward",
-      backend_config="{\"activation_mode\":\"2\",\"conv_result_scale\":1,\"side_input_scale\":0,\"algorithm\":{
-        \"algo_id\":\"38\",\"math_type\":\"DEFAULT_MATH\",\"tuning_knobs\":{\"14\":\"5\",\"13\":\"1\",\"23\":\"0\",\"2\":\"1\"},
-        \"is_cudnn_frontend\":true,\"workspace_size\":\"0\"}}"
-  ROOT get-tuple-element.1 = s8[4,48,48,64]{3,2,1,0} get-tuple-element(cudnn-conv-bias-activation.3), index=0
-})";
-  constexpr char kHloVectorized[] = R"(
-HloModule TestModule, entry_computation_layout={(s8[4,48,48,64]{3,2,1,0},s8[64,3,3,64]{3,2,1,0},s8[64]{0})->s8[4,48,48,64]{3,2,1,0}}
-
-ENTRY TestComputation {
-  input = s8[4,48,48,64]{3,2,1,0} parameter(0)
-  bitcast.36 = s8[4,48,48,2,32]{4,3,2,1,0} bitcast(input)
-  transpose = s8[4,2,48,48,32]{4,3,2,1,0} transpose(bitcast.36), dimensions={0,3,1,2,4}
-  filter = s8[64,3,3,64]{3,2,1,0} parameter(1)
-  bitcast.21 = s8[64,3,3,2,32]{4,3,2,1,0} bitcast(filter)
-  transpose.4 = s8[64,2,3,3,32]{4,3,2,1,0} transpose(bitcast.21), dimensions={0,3,1,2,4}
-  bitcast.24 = s8[8,4,2,2,3,3,8,4]{7,6,5,4,3,2,1,0} bitcast(transpose.4)
-  transpose.5 = s8[2,3,3,8,2,8,4,4]{7,6,5,4,3,2,1,0} transpose(bitcast.24), dimensions={3,4,5,0,2,6,1,7}
-  bitcast.28 = s8[64,2,3,3,32]{4,3,2,1,0} bitcast(transpose.5)
-  bias = s8[64]{0} parameter(2)
-  convert.2 = f32[64]{0} convert(bias)
-  bitcast.33 = f32[2,4,2,4]{3,2,1,0} bitcast(convert.2)
-  transpose.6 = f32[2,2,4,4]{3,2,1,0} transpose(bitcast.33), dimensions={0,2,1,3}
-  bitcast.37 = f32[64]{0} bitcast(transpose.6)
-  cudnn-conv-bias-activation.4 = (s8[4,2,48,48,32]{4,3,2,1,0}, u8[51328]{0}) custom-call(transpose, bitcast.28, bitcast.37),
-      window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target="__cudnn$convBiasActivationForward",
-      backend_config="{\"activation_mode\":\"2\",\"conv_result_scale\":1,\"side_input_scale\":0,\"algorithm\":{
-        \"algo_id\":\"7\",\"math_type\":\"DEFAULT_MATH\",\"tuning_knobs\":{\"7\":\"3\",\"2\":\"0\",\"5\":\"4\",\"6\":\"4\",\"4\":\"2\",\"21\":\"0\"},
-        \"is_cudnn_frontend\":true,\"workspace_size\":\"51328\"},\"reordered_int8_nchw_vect\":true}"
-  get-tuple-element.6 = s8[4,2,48,48,32]{4,3,2,1,0} get-tuple-element(cudnn-conv-bias-activation.4), index=0
-  transpose.1 = s8[4,48,48,2,32]{4,3,2,1,0} transpose(get-tuple-element.6), dimensions={0,2,3,1,4}
-  ROOT bitcast.1 = s8[4,48,48,64]{3,2,1,0} bitcast(transpose.1)
-})";
-  EXPECT_TRUE(RunAndCompareTwoModules(kHloBase, kHloVectorized, ErrorSpec{0, 0},
-                                      /*run_hlo_passes=*/false));
-}
-
 class HloCompareModulesTest : public HloTestBase {
  public:
   HloCompareModulesTest(std::string hlo_base, std::string hlo_test)
diff --git a/xla/translate/mhlo_to_lhlo_with_xla/mhlo_to_lhlo_with_xla.cc b/xla/translate/mhlo_to_lhlo_with_xla/mhlo_to_lhlo_with_xla.cc
index a2c5e748f..bce80a0bd 100644
--- a/xla/translate/mhlo_to_lhlo_with_xla/mhlo_to_lhlo_with_xla.cc
+++ b/xla/translate/mhlo_to_lhlo_with_xla/mhlo_to_lhlo_with_xla.cc
@@ -1200,10 +1200,10 @@ LhloDialectEmitter::EmitDnnConvolutionReorderVectorized(
     }
 
     CHECK_EQ(shape.rank(), 5);
-    CHECK_EQ(shape.dimensions(4), 32);
+    CHECK_EQ(shape.dimensions_minor(0), 32);
     llvm::SmallVector<int64_t, 4> nchw = {
-        shape.dimensions(0), shape.dimensions(1) * 32, shape.dimensions(2),
-        shape.dimensions(3)};
+        shape.dimensions_minor(4), shape.dimensions_minor(3) * 32,
+        shape.dimensions_minor(2), shape.dimensions_minor(1)};
     op->setAttr("filter_dims", GetI64DenseElementsAttr(nchw));
 
     return op.getOperation();
diff --git a/xla/xla.proto b/xla/xla.proto
index e34cf53cb..3357cd2db 100644
--- a/xla/xla.proto
+++ b/xla/xla.proto
@@ -465,8 +465,6 @@ message DebugOptions {
 
   bool xla_gpu_enable_triton_gemm = 188;
 
-  bool xla_gpu_enable_cudnn_int8x32_convolution_reordering = 189;
-
   bool xla_gpu_triton_gemm_any = 190;
 
   // Next id: 192
