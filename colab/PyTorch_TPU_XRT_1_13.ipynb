{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch TPU XRT 1.13",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "code",
        "id": "lIYdn1woOS1n",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fdBz0cmnDkLJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Copyright 2019 Google LLC.\n",
        "SPDX-License-Identifier: Apache-2.0"
      ]
    },
    {
      "metadata": {
        "id": "IQG3y7IGvc7o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install \\\n",
        "  http://storage.googleapis.com/pytorch-tpu-releases/tf-1.13/torch-1.0.0a0+1d94a2b-cp36-cp36m-linux_x86_64.whl  \\\n",
        "  http://storage.googleapis.com/pytorch-tpu-releases/tf-1.13/torch_xla-0.1+5622d42-cp36-cp36m-linux_x86_64.whl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "brEkCkFI-Hmy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch_xla\n",
        "\n",
        "class XlaMulAdd(nn.Module):                                                                                             \n",
        "  def forward(self, x, y):                                                                                            \n",
        "    return x * y + y                                                                                                \n",
        "\n",
        "# Inputs and output to/from XLA models are always in replicated mode. The shapes\n",
        "# are [NUM_REPLICAS][NUM_VALUES]. A non replicated, single core, execution will\n",
        "# has NUM_REPLICAS == 1, but retain the same shape rank.                                                                                                                               \n",
        "x = torch.rand(3, 5)                                                                                                    \n",
        "y = torch.rand(3, 5)                                                                                                    \n",
        "model = XlaMulAdd()                                                                                                     \n",
        "traced_model = torch.jit.trace(model, (x, y))                                                                             \n",
        "xla_model = torch_xla._XLAC.XlaModule(traced_model)                                                             \n",
        "output_xla = xla_model((torch_xla._XLAC.XLATensor(x), torch_xla._XLAC.XLATensor(y)))                                               \n",
        "expected = model(x, y)\n",
        "print(output_xla[0][0].to_tensor().data)\n",
        "print(expected.data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}